defaults:
  - wrangl_experiment
  - _self_

early_stopping:
    monitor: 'global/eval_running_reward'
    mode: 'max'
time_key: 'global/env_train_steps'

##########
# training
##########
localdir: "${dsave}/peers/${peer}"
local_name: "${name}-${peer}"
peer: 0
debug: false
max_steps: 50_000_000
autoresume: false

device: 'cuda:0'
actor_batch_size: 32
baseline_cost: 0.5
batch_size: 8
connect: 127.0.0.1:4431
discounting: 0.99
entropy_cost: 0.0006
env: null
grad_clip_norm: 40
normalize_advantages: true
appo_clip_baseline: 1.0  # null to disable
appo_clip_policy: 0.1  # null to disable
num_actor_batches: 2
num_actor_cpus: 4
learning_rate: 0.0006
unroll_length: 20
stateful: false
virtual_batch_size: 32
reward_clip: 1.0
reward_scale: 1.0
warmup: 0
val_check_interval: 100  # how often to evaluate in seconds
eval_steps: 10000  # how many steps to evaluate for
